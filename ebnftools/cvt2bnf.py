#!/usr/bin/env python3

from ebnftools import ebnfast, ebnfgrammar
from ebnftools.ebnfanno import Anno, SExprList, Symbol
from ebnftools.ebnfgen import isfinite, count, generate2, flatten
from ebnftools.convert.bnf import EBNF2BNF, get_strings, get_charclass, LiteralRewriter, CharClassRewriter
from ebnftools.convert.tokens import TokenRegistry, TknLiteral, TknRegExp, TknCharClass
import argparse
import copy
import pathlib
import sys

def sanity_check(orig, new, paranoid = True):
    orig_as_dict = dict([(r.lhs.value, r.rhs) for r in orig])
    new_as_dict = dict([(r.lhs.value, r.rhs) for r in new])

    for k in orig_as_dict:
        if k not in new_as_dict:
            print(f"ERROR: {k} is *not* in bnf", file=sys.stderr)
        else:
            if isfinite(orig_as_dict, orig_as_dict[k]) and isfinite(new_as_dict, new_as_dict[k]):
                # check for weak-equivalence by counting
                # ideally we would check the strings in the language as well

                co = count(orig_as_dict, orig_as_dict[k])
                cn = count(new_as_dict, new_as_dict[k])
                if co != cn:
                    print(f"ERROR: Count for rule {k}: EBNF count {co} != BNF count {cn}", file=sys.stderr)
                else:
                    print(f"INFO: {k} is okay [{cn} == {co}]", file=sys.stderr)

                if paranoid:
                    s1 = set(generate2(orig_as_dict, orig_as_dict[k]))
                    s2 = set(generate2(new_as_dict, new_as_dict[k]))

                    if s1 != s2:
                        print(f"ERROR: Generated strings for {k} don't match: {s1 - s2} and {s2 - s1}", file=sys.stderr)
                    else:
                        print(f"INFO: Generated strings for {k} match", file=sys.stderr)
            else:
                print(f"ERROR: Can't check {k} for weak equivalence, contains infinite productions", file=sys.stderr)

def _add_token(treg, prefix, value, k = 0):
    try_token_name = prefix + f"_{k}"

    while try_token_name in treg.tokens:
        try_token_name = prefix + f"_{k}"
        k += 1

    treg.add(try_token_name, value)

    return k

def tokenize_cc_literals(bnf, treg):
    all_cc = get_charclass(bnf)

    k = 0
    for s in all_cc:
        sk = TknCharClass(s[1:-1])

        if sk.key() not in treg.v2n:
            k = _add_token(treg, "TOKEN_CC", sk, k)

    return treg

def tokenize_string_literals(bnf, treg):
    all_strings = get_strings(bnf)

    k = 0
    for s in all_strings:
        if s == '': continue # empty strings are handled differently

        sk = TknLiteral(s)
        if sk.key() not in treg.v2n:
            k = _add_token(treg, "TOKEN_STR", sk, k)

    return treg

def status_message(m):
    print(m, file=sys.stderr)

def reorder(tr):
    existing = set(tr.read_order)
    new_order = []
    for k in tr.n2v:
        if k in existing: continue
        v = tr.n2v[k]
        if isinstance(v, TknLiteral):
            new_order.append((k, (0, -len(v.value)))) # string literals
        elif isinstance(v, TknCharClass):
            new_order.append((k, (1,)))
        elif isinstance(v, TknRegExp):
            new_order.append((k, (2,)))

    new_order.sort(key = lambda x: x[1])
    if len(tr.read_order):
        tr.read_order.extend([x[0] for x in new_order])
    else:
        tr.read_order = list([x[0] for x in new_order])

if __name__ == "__main__":
    p = argparse.ArgumentParser(description="Convert EBNF2BNF")
    p.add_argument("ebnf")
    p.add_argument("tokens", nargs="?", help="File containing tokens (used as input/output)")
    p.add_argument("--check", action="store_true",
                   help="Check that resulting BNF is `equivalent' (when possible)")
    p.add_argument("--token-out", help="Don't rewrite input tokens file", default=None)

    sys.setrecursionlimit(15000)

    args = p.parse_args()

    pr = ebnfgrammar.EBNFAnnotatedGrammar()
    with open(args.ebnf, "r") as f:
        gr = f.read()

    status_message("parsing grammar")
    pr.parse(gr)

    status_message("computing treepos")
    # TODO: process NAMEPOS
    for r in pr.rules:
        pr.compute_treepos(r)

    status_message("converting to BNF")
    rules = list(pr.rules)
    orig_rules = copy.deepcopy(rules)
    xf = EBNF2BNF()
    bnf = xf.visit_RuleList(rules)

    status_message("creating new grammar")
    out = ebnfgrammar.EBNFAnnotatedGrammar()
    out.from_rules(bnf, pr.anno) # BUGGY in more ways than one, pass unannotated files

    print(f"/* autogenerated file from {args.ebnf} */")
    if args.tokens:
        tokens = {}
        tkns = pathlib.Path(args.tokens)
        treg = TokenRegistry(args.tokens)
        if tkns.exists(): treg.read()

        treg = tokenize_string_literals(bnf, treg)
        treg = tokenize_cc_literals(bnf, treg)
        reorder(treg)
        treg.write_ordered(args.token_out)

        rwt = LiteralRewriter()
        rwt.rewrite(bnf, treg.v2n)

        ccrwt = CharClassRewriter()
        ccrwt.rewrite(bnf, treg.v2n)

        for t, s in treg.n2v.items():
            if not isinstance(s, TknRegExp):
                # note that s are subclasses of String/CharClass
                print(ebnfast.Rule(ebnfast.Symbol(t), s))
            else:
                # this is usually not a problem since the token will
                # be in the lexer anyway, but it would be nice to see
                # if the regexp can be converted to EBNF
                print(f"Regular expressions literals in EBNF are not supported, omitting token '{t} ::= {s}'", file=sys.stderr)

    status_message("writing BNF")
    for r in out.raw:
        print(r)

    status_message("writing treepos map")
    map_anno = []
    for r in out.rules:
        #print(r)
        #for x in out.get_treepos(r):
        #    print("\t", x)
        #print("***")
        out.compute_treepos(r)
        for x in out.get_treepos(r):
            if x.previous is not None:
                if x.previous != x.current:
                    a = Anno("TREEPOS_MAP",
                             [SExprList(Symbol(x.previous[0]), SExprList(*x.previous[1])),
                              SExprList(Symbol(x.current[0]), SExprList(*x.current[1]))]
                    )
                    print("\t", x.previous, "=>", x.current, file=sys.stderr)
                    map_anno.append(a)

            if x.original is not None:
                if x.original != x.current:
                    a = Anno("TREEPOS_MAP",
                             [SExprList(Symbol(x.original[0]), SExprList(*x.original[1])),
                              SExprList(Symbol(x.current[0]), SExprList(*x.current[1]))])
                    print("\t", x.original, "=>", x.current, file=sys.stderr)
                    map_anno.append(a)

    for a in map_anno:
        print(str(a))

    status_message("checking")
    if args.check:
        sanity_check(orig_rules, bnf, True)
